{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a skeleton class and associated test functions for the `fprop`, `bprop` and `grads_wrt_params` methods of the ConvolutionalLayer class are included.\n",
    "\n",
    "The test functions assume that in your implementation of `fprop` for the convolutional layer, outputs are calculated only for 'valid' overlaps of the kernel filters with the input - i.e. without any padding.\n",
    "\n",
    "It is also assumed that if convolutions with non-unit strides are implemented the default behaviour is to take unit-strides, with the test cases only correct for unit strides in both directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three test functions are defined in the cell below. All the functions take as first argument the *class* corresponding to the convolutional layer implementation to be tested (**not** an instance of the class). It is assumed the class being tested has an `__init__` method with at least all of the arguments defined in the skeleton definition above. A boolean second argument to each function can be used to specify if the layer implements a cross-correlation or convolution based operation (see note in [seventh lecture slides](http://www.inf.ed.ac.uk/teaching/courses/mlp/2016/mlp07-cnn.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_conv_layer_fprop(layer_class, do_cross_correlation=False):\n",
    "    \"\"\"Tests `fprop` method of a convolutional layer.\n",
    "    \n",
    "    Checks the outputs of `fprop` method for a fixed input against known\n",
    "    reference values for the outputs and raises an AssertionError if\n",
    "    the outputted values are not consistent with the reference values. If\n",
    "    tests are all passed returns True.\n",
    "    \n",
    "    Args:\n",
    "        layer_class: Convolutional layer implementation following the \n",
    "            interface defined in the provided skeleton class.\n",
    "        do_cross_correlation: Whether the layer implements an operation\n",
    "            corresponding to cross-correlation (True) i.e kernels are\n",
    "            not flipped before sliding over inputs, or convolution\n",
    "            (False) with filters being flipped.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: Raised if output of `layer.fprop` is inconsistent \n",
    "            with reference values either in shape or values.\n",
    "    \"\"\"\n",
    "    inputs = np.arange(96).reshape((2, 3, 4, 4))\n",
    "    kernels = np.arange(-12, 12).reshape((2, 3, 2, 2))\n",
    "    if do_cross_correlation:\n",
    "        kernels = kernels[:, :, ::-1, ::-1]\n",
    "    biases = np.arange(2)\n",
    "    true_output = np.array(\n",
    "        [[[[ -958., -1036., -1114.],\n",
    "           [-1270., -1348., -1426.],\n",
    "           [-1582., -1660., -1738.]],\n",
    "          [[ 1707.,  1773.,  1839.],\n",
    "           [ 1971.,  2037.,  2103.],\n",
    "           [ 2235.,  2301.,  2367.]]],\n",
    "         [[[-4702., -4780., -4858.],\n",
    "           [-5014., -5092., -5170.],\n",
    "           [-5326., -5404., -5482.]],\n",
    "          [[ 4875.,  4941.,  5007.],\n",
    "           [ 5139.,  5205.,  5271.],\n",
    "           [ 5403.,  5469.,  5535.]]]]\n",
    "    )\n",
    "    \n",
    "    layer = layer_class(\n",
    "        num_input_channels=kernels.shape[1], \n",
    "        num_output_channels=kernels.shape[0], \n",
    "        input_dim_1=inputs.shape[2], \n",
    "        input_dim_2=inputs.shape[3],\n",
    "        kernel_dim_1=kernels.shape[2],\n",
    "        kernel_dim_2=kernels.shape[3]\n",
    "    )\n",
    "    layer.params = [kernels, biases]\n",
    "    layer_output = layer.fprop(inputs)\n",
    "    \n",
    "    assert layer_output.shape == true_output.shape, (\n",
    "        'Layer fprop gives incorrect shaped output. '\n",
    "        'Correct shape is \\n\\n{0}\\n\\n but returned shape is \\n\\n{1}.'\n",
    "        .format(true_output.shape, layer_output.shape)\n",
    "    )\n",
    "    assert np.allclose(layer_output, true_output), (\n",
    "        'Layer fprop does not give correct output. '\n",
    "        'Correct output is \\n\\n{0}\\n\\n but returned output is \\n\\n{1}\\n\\n difference is \\n\\n{2}.'\n",
    "        .format(true_output, layer_output, true_output-layer_output)\n",
    "    )\n",
    "    return True\n",
    "\n",
    "def test_conv_layer_bprop(layer_class, do_cross_correlation=False):\n",
    "    \"\"\"Tests `bprop` method of a convolutional layer.\n",
    "    \n",
    "    Checks the outputs of `bprop` method for a fixed input against known\n",
    "    reference values for the gradients with respect to inputs and raises \n",
    "    an AssertionError if the returned values are not consistent with the\n",
    "    reference values. If tests are all passed returns True.\n",
    "    \n",
    "    Args:\n",
    "        layer_class: Convolutional layer implementation following the \n",
    "            interface defined in the provided skeleton class.\n",
    "        do_cross_correlation: Whether the layer implements an operation\n",
    "            corresponding to cross-correlation (True) i.e kernels are\n",
    "            not flipped before sliding over inputs, or convolution\n",
    "            (False) with filters being flipped.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: Raised if output of `layer.bprop` is inconsistent \n",
    "            with reference values either in shape or values.\n",
    "    \"\"\"\n",
    "    inputs = np.arange(96).reshape((2, 3, 4, 4))\n",
    "    kernels = np.arange(-12, 12).reshape((2, 3, 2, 2))\n",
    "    if do_cross_correlation:\n",
    "        kernels = kernels[:, :, ::-1, ::-1]\n",
    "    biases = np.arange(2)\n",
    "    grads_wrt_outputs = np.arange(-20, 16).reshape((2, 2, 3, 3))\n",
    "    outputs = np.array(\n",
    "        [[[[ -958., -1036., -1114.],\n",
    "           [-1270., -1348., -1426.],\n",
    "           [-1582., -1660., -1738.]],\n",
    "          [[ 1707.,  1773.,  1839.],\n",
    "           [ 1971.,  2037.,  2103.],\n",
    "           [ 2235.,  2301.,  2367.]]],\n",
    "         [[[-4702., -4780., -4858.],\n",
    "           [-5014., -5092., -5170.],\n",
    "           [-5326., -5404., -5482.]],\n",
    "          [[ 4875.,  4941.,  5007.],\n",
    "           [ 5139.,  5205.,  5271.],\n",
    "           [ 5403.,  5469.,  5535.]]]]\n",
    "    )\n",
    "    true_grads_wrt_inputs = np.array(\n",
    "      [[[[ 147.,  319.,  305.,  162.],\n",
    "         [ 338.,  716.,  680.,  354.],\n",
    "         [ 290.,  608.,  572.,  294.],\n",
    "         [ 149.,  307.,  285.,  144.]],\n",
    "        [[  23.,   79.,   81.,   54.],\n",
    "         [ 114.,  284.,  280.,  162.],\n",
    "         [ 114.,  272.,  268.,  150.],\n",
    "         [  73.,  163.,  157.,   84.]],\n",
    "        [[-101., -161., -143.,  -54.],\n",
    "         [-110., -148., -120.,  -30.],\n",
    "         [ -62.,  -64.,  -36.,    6.],\n",
    "         [  -3.,   19.,   29.,   24.]]],\n",
    "       [[[  39.,   67.,   53.,   18.],\n",
    "         [  50.,   68.,   32.,   -6.],\n",
    "         [   2.,  -40.,  -76.,  -66.],\n",
    "         [ -31.,  -89., -111.,  -72.]],\n",
    "        [[  59.,  115.,  117.,   54.],\n",
    "         [ 114.,  212.,  208.,   90.],\n",
    "         [ 114.,  200.,  196.,   78.],\n",
    "         [  37.,   55.,   49.,   12.]],\n",
    "        [[  79.,  163.,  181.,   90.],\n",
    "         [ 178.,  356.,  384.,  186.],\n",
    "         [ 226.,  440.,  468.,  222.],\n",
    "         [ 105.,  199.,  209.,   96.]]]])\n",
    "    layer = layer_class(\n",
    "        num_input_channels=kernels.shape[1], \n",
    "        num_output_channels=kernels.shape[0], \n",
    "        input_dim_1=inputs.shape[2], \n",
    "        input_dim_2=inputs.shape[3],\n",
    "        kernel_dim_1=kernels.shape[2],\n",
    "        kernel_dim_2=kernels.shape[3]\n",
    "    )\n",
    "    layer.params = [kernels, biases]\n",
    "    layer_grads_wrt_inputs = layer.bprop(inputs, outputs, grads_wrt_outputs)\n",
    "    assert layer_grads_wrt_inputs.shape == true_grads_wrt_inputs.shape, (\n",
    "        'Layer bprop returns incorrect shaped array. '\n",
    "        'Correct shape is \\n\\n{0}\\n\\n but returned shape is \\n\\n{1}.'\n",
    "        .format(true_grads_wrt_inputs.shape, layer_grads_wrt_inputs.shape)\n",
    "    )\n",
    "    assert np.allclose(layer_grads_wrt_inputs, true_grads_wrt_inputs), (\n",
    "        'Layer bprop does not return correct values. '\n",
    "        'Correct output is \\n\\n{0}\\n\\n but returned output is \\n\\n{1}\\n\\n difference is \\n\\n{2}'\n",
    "        .format(true_grads_wrt_inputs, layer_grads_wrt_inputs, layer_grads_wrt_inputs-true_grads_wrt_inputs)\n",
    "    )\n",
    "    return True\n",
    "\n",
    "def test_conv_layer_grad_wrt_params(\n",
    "        layer_class, do_cross_correlation=False):\n",
    "    \"\"\"Tests `grad_wrt_params` method of a convolutional layer.\n",
    "    \n",
    "    Checks the outputs of `grad_wrt_params` method for fixed inputs \n",
    "    against known reference values for the gradients with respect to \n",
    "    kernels and biases, and raises an AssertionError if the returned\n",
    "    values are not consistent with the reference values. If tests\n",
    "    are all passed returns True.\n",
    "    \n",
    "    Args:\n",
    "        layer_class: Convolutional layer implementation following the \n",
    "            interface defined in the provided skeleton class.\n",
    "        do_cross_correlation: Whether the layer implements an operation\n",
    "            corresponding to cross-correlation (True) i.e kernels are\n",
    "            not flipped before sliding over inputs, or convolution\n",
    "            (False) with filters being flipped.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: Raised if output of `layer.bprop` is inconsistent \n",
    "            with reference values either in shape or values.\n",
    "    \"\"\"\n",
    "    inputs = np.arange(96).reshape((2, 3, 4, 4))\n",
    "    kernels = np.arange(-12, 12).reshape((2, 3, 2, 2))\n",
    "    biases = np.arange(2)\n",
    "    grads_wrt_outputs = np.arange(-20, 16).reshape((2, 2, 3, 3))\n",
    "    true_kernel_grads = np.array(\n",
    "        [[[[ -240.,  -114.],\n",
    "         [  264.,   390.]],\n",
    "        [[-2256., -2130.],\n",
    "         [-1752., -1626.]],\n",
    "        [[-4272., -4146.],\n",
    "         [-3768., -3642.]]],\n",
    "       [[[ 5268.,  5232.],\n",
    "         [ 5124.,  5088.]],\n",
    "        [[ 5844.,  5808.],\n",
    "         [ 5700.,  5664.]],\n",
    "        [[ 6420.,  6384.],\n",
    "         [ 6276.,  6240.]]]])\n",
    "    if do_cross_correlation:\n",
    "        kernels = kernels[:, :, ::-1, ::-1]\n",
    "        true_kernel_grads = true_kernel_grads[:, :, ::-1, ::-1]\n",
    "    true_bias_grads = np.array([-126.,   36.])\n",
    "    layer = layer_class(\n",
    "        num_input_channels=kernels.shape[1], \n",
    "        num_output_channels=kernels.shape[0], \n",
    "        input_dim_1=inputs.shape[2], \n",
    "        input_dim_2=inputs.shape[3],\n",
    "        kernel_dim_1=kernels.shape[2],\n",
    "        kernel_dim_2=kernels.shape[3]\n",
    "    )\n",
    "    layer.params = [kernels, biases]\n",
    "    layer_kernel_grads, layer_bias_grads = (\n",
    "        layer.grads_wrt_params(inputs, grads_wrt_outputs))\n",
    "    assert layer_kernel_grads.shape == true_kernel_grads.shape, (\n",
    "        'grads_wrt_params gives incorrect shaped kernel gradients output. '\n",
    "        'Correct shape is \\n\\n{0}\\n\\n but returned shape is \\n\\n{1}.'\n",
    "        .format(true_kernel_grads.shape, layer_kernel_grads.shape)\n",
    "    )\n",
    "    assert np.allclose(layer_kernel_grads, true_kernel_grads), (\n",
    "        'grads_wrt_params does not give correct kernel gradients output. '\n",
    "        'Correct output is \\n\\n{0}\\n\\n but returned output is \\n\\n{1}.'\n",
    "        .format(true_kernel_grads, layer_kernel_grads)\n",
    "    )\n",
    "    assert layer_bias_grads.shape == true_bias_grads.shape, (\n",
    "        'grads_wrt_params gives incorrect shaped bias gradients output. '\n",
    "        'Correct shape is \\n\\n{0}\\n\\n but returned shape is \\n\\n{1}.'\n",
    "        .format(true_bias_grads.shape, layer_bias_grads.shape)\n",
    "    )\n",
    "    assert np.allclose(layer_bias_grads, true_bias_grads), (\n",
    "        'grads_wrt_params does not give correct bias gradients output. '\n",
    "        'Correct output is \\n\\n{0}\\n\\n but returned output is \\n\\n{1}.'\n",
    "        .format(true_bias_grads, layer_bias_grads)\n",
    "    )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of using the test functions if given in the cell below. This assumes you implement a convolution (rather than cross-correlation) operation. If the implementation is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import ConvolutionalLayer\n",
    "fprop_correct = test_conv_layer_fprop(ConvolutionalLayer, False)\n",
    "bprop_correct = test_conv_layer_bprop(ConvolutionalLayer, False)\n",
    "grads_wrt_param_correct = test_conv_layer_grad_wrt_params(ConvolutionalLayer, False)\n",
    "if fprop_correct and grads_wrt_param_correct and bprop_correct:\n",
    "    print('All tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.layers import Layer\n",
    "class test_MaxPoolingLayer(Layer):\n",
    "\n",
    "    def __init__(self, num_input_channels,\n",
    "                 input_dim_1, input_dim_2,\n",
    "                 kernel_dim_1, kernel_dim_2\n",
    "                ):\n",
    "\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.input_dim_1 = input_dim_1\n",
    "        self.input_dim_2 = input_dim_2\n",
    "        self.kernel_dim_1 = kernel_dim_1\n",
    "        self.kernel_dim_2 = kernel_dim_2\n",
    "        self.kernels_shape = (\n",
    "            kernel_dim_1, kernel_dim_2\n",
    "        )\n",
    "        self.inputs_shape = (\n",
    "            None, num_input_channels, input_dim_1, input_dim_2\n",
    "        )\n",
    "        assert input_dim_1%kernel_dim_1==0 and input_dim_1%kernel_dim_1==0, (\n",
    "            'inputs shape ({0},{1}) is not integral multiple of kernel shape ({2},{3})'\n",
    "            .format(input_dim_1, input_dim_2, kernel_dim_1, kernel_dim_2)\n",
    "        )\n",
    "        self.output_dim_1 = input_dim_1//kernel_dim_1\n",
    "        self.output_dim_2 = input_dim_2//kernel_dim_2\n",
    "\n",
    "        self.cache = None\n",
    "\n",
    "    def fprop(self, inputs):\n",
    "        \n",
    "        kernel_shape = (self.kernel_dim_1, self.kernel_dim_2)\n",
    "        unroll_shape = inputs.shape[:2] + tuple(np.floor_divide(inputs.shape[-2:], kernel_shape)) + kernel_shape\n",
    "        # shape (batch_size, num_input_channel, output_dim1, output_dim2, kernel_dim1, kernel_dim2)\n",
    "        strides = inputs.strides[:2]+tuple(np.multiply(inputs.strides[-2:], kernel_shape))+inputs.strides[-2:]\n",
    "\n",
    "        inputs_unroll = np.lib.stride_tricks.as_strided(inputs,unroll_shape,strides)\n",
    "        outputs = inputs_unroll.max(axis=(4,5))\n",
    "        mask = inputs_unroll == outputs.reshape(outputs.shape+(1,1))\n",
    "\n",
    "        self.cache = (mask,)\n",
    "        return outputs\n",
    "\n",
    "    def bprop(self, inputs, outputs, grads_wrt_outputs):\n",
    "        \n",
    "        mask, = self.cache\n",
    "        kernel_shape = (self.kernel_dim_1, self.kernel_dim_2)\n",
    "        grads_wrt_outputs_unroll = grads_wrt_outputs.reshape(grads_wrt_outputs.shape+(1,1))*mask\n",
    "        roll_shape = grads_wrt_outputs_unroll.shape[:2] + \\\n",
    "            tuple(np.multiply(grads_wrt_outputs.shape[-2:], kernel_shape))\n",
    "        strides = grads_wrt_outputs_unroll.strides[:2] + grads_wrt_outputs_unroll.strides[-2:]\n",
    "        grads_wrt_inputs = np.lib.stride_tricks.as_strided(grads_wrt_outputs_unroll,roll_shape,strides)\n",
    "        return grads_wrt_inputs\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            'MaxPollingLayer(\\n'\n",
    "            '    num_channels={0},\\n'\n",
    "            '    input_dim_1={1}, input_dim_2={2},\\n'\n",
    "            '    kernel_dim_1={3}, kernel_dim_2={4}\\n'\n",
    "            ')'\n",
    "            .format(self.num_input_channels,\n",
    "                    self.input_dim_1, self.input_dim_2, self.kernel_dim_1,\n",
    "                    self.kernel_dim_2)\n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.layers import MaxPoolingLayer\n",
    "inputs = np.random.rand(3,3,6,6)\n",
    "mply = test_MaxPoolingLayer(3,8,8,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mply.fprop(inputs)\n",
    "iin = mply.bprop(inputs, out, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mply.fprop(inputs-iin)\n",
    "y = mply.bprop(iin-inputs, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.        ,  0.37009796,  0.65927395,  0.        ,  0.81089078,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.33871451,  0.        ,  0.        ,\n",
       "           0.34748087],\n",
       "         [ 0.2067875 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.40695187,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.52695803,  0.        ,  0.        ,  0.79367069,\n",
       "           0.        ]],\n",
       "\n",
       "        [[ 0.25463677,  0.        ,  0.        ,  0.        ,  0.73894968,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.63959774,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.66113097,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.6144259 ,  0.        ,  0.        ,\n",
       "           0.20257871],\n",
       "         [ 0.        ,  0.6073049 ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.43277833,  0.        ,  0.        ,\n",
       "           0.45717684]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.69694528,  0.        ,  0.54797008,  0.        ,  0.        ,\n",
       "           0.38140025],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.70543393,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.63448319,  0.        ,  0.        ,  0.24490756,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.05963101,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.53806702,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.38056292]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.46578283,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.57016239,  0.        ,  0.84187128,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.67990146,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.8192686 ,  0.        ,  0.        ,\n",
       "           0.59301555],\n",
       "         [ 0.        ,  0.        ,  0.91884049,  0.        ,  0.6288956 ,\n",
       "           0.        ],\n",
       "         [ 0.64359051,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.44096369,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.79554414,  0.        ,  0.        ,  0.        ,  0.74909887,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.94083961,  0.        ,  0.56482275,  0.6421815 ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.64630218],\n",
       "         [ 0.        ,  0.45075237,  0.        ,  0.85933267,  0.        ,\n",
       "           0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.89136072,  0.58594921,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.71783305,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.47102975,  0.        ,  0.        ,  0.97184588,  0.84475252,\n",
       "           0.        ],\n",
       "         [ 0.77216427,  0.        ,  0.        ,  0.        ,  0.88340018,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.89069042,  0.        ,\n",
       "           0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.77608465,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.24625419,  0.        ,  0.        ,\n",
       "           0.50099535],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.72376764,  0.8489224 ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.54746817,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.57373571,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.34685215,  0.29509181,  0.        ,  0.        ,\n",
       "           0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.32796118,  0.58015689,  0.        ,  0.66380745,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.35616007,  0.79092106,  0.        ,  0.50184116,\n",
       "           0.        ],\n",
       "         [ 0.6265802 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.64472555],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.79321569,  0.        ,\n",
       "           0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.1802482 ,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.3440025 ,  0.        ,  0.        ,  0.65933904,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.53828063],\n",
       "         [ 0.68809185,  0.        ,  0.53628847,  0.        ,  0.        ,\n",
       "           0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "           0.87760998],\n",
       "         [ 0.        ,  0.81985692,  0.06783534,  0.        ,  0.        ,\n",
       "           0.        ]]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
